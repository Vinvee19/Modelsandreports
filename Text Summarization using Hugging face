import gradio as gr
from transformers import PegasusTokenizer, PegasusForConditionalGeneration
import torch

# Load model and tokenizer
model_name = "google/pegasus-xsum"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define summarization function
def summarize_text(text, max_length=60, min_length=10, num_beams=4):
    inputs = tokenizer(text, truncation=True, padding="longest", return_tensors="pt").to(device)
    summary_ids = model.generate(inputs.input_ids, max_length=max_length, min_length=min_length, num_beams=num_beams, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Create Gradio interface
iface = gr.Interface(fn=summarize_text, inputs="text", outputs="text")
iface.launch()
